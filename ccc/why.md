# 用外行術語解釋 llm.c

訓練大型語言模型 (LLM)（例如 ChatGPT）涉及大量程式碼和複雜性。

例如，典型的 LLM 訓練課程可能會使用 PyTorch 深度學習庫。 PyTorch 相當複雜，因為它實現了一個非常通用的張量抽象（一種排列和操作保存神經網路參數和激活的數字數組的方法），一個非常通用的反向傳播Autograd 引擎（訓練神經網路參數的演算法） ），以及您可能希望在神經網路中使用的大量深度學習層。 PyTorch 專案有 11,449 個檔案中的 3,327,184 行程式碼。
最重要的是，PyTorch 是用 Python 寫的，Python 本身就是一種非常高階的語言。 您必須執行 Python 解釋器將訓練程式碼轉換為低階電腦指令。 例如，執行此轉換的 cPython 專案包含 4,306 個檔案中的 2,437,955 行程式碼。
我正在刪除所有這些複雜性，並將 LLM 培訓簡化為其最基本的要素，以非常低階的語言 (C) 直接與電腦對話，並且沒有其他程式庫依賴項。 下面唯一的抽像是彙編程式碼本身。 我認為人們會感到驚訝的是，與上述相比，訓練像 GPT-2 這樣的 LLM 實際上只需要在單一檔案中使用大約 1000 行 C 程式碼。 我透過直接在 C 中實現 GPT-2 的神經網路訓練演算法來實現這種壓縮。這很困難，因為你必須詳細了解訓練演算法，能夠導出所有層的反向傳播的所有前向和後向傳遞，並非常仔細地實現所有數組索引計算，因為您沒有可用的PyTorch 張量抽象。 所以安排起來是一件非常脆弱的事情，但是一旦你這樣做了，並且通過再次檢查 PyTorch 來驗證正確性，你就會得到一些非常簡單、小而且在我看來相當漂亮的東西。
好吧，為什麼人們不一直這樣做呢？

第一：你放棄了很大的彈性。 如果你想改變你的神經網絡，在 PyTorch 中你可能需要改變一行程式碼。 在 llm.c 中，更改很可能會涉及更多程式碼，可能會更加困難，並且需要更多專業知識。 例如。 如果它是一個新的操作，你可能需要做一些微積分，並編寫它的前向傳播和後向傳播以進行反向傳播，並確保它在數學上是正確的。

第二：你正在放棄速度，至少一開始是這樣。 天下沒有免費的午餐 - 您不應該指望僅 1,000 行就能達到最先進的速度。 PyTorch 在後台做了很多工作，以確保神經網路非常有效率。 不僅所有 Tensor 操作都非常仔細地調用最高效的 CUDA 內核，而且還有 torch.compile 等功能，它可以進一步分析和優化您的神經網路以及它如何最有效地在您的電腦上運行。 現在，原則上，llm.c 應該能夠呼叫所有相同的核心並直接執行。 但這需要更多的工作和注意力，就像 (1) 一樣，如果您更改神經網路或正在運行的電腦的任何內容，您可能必須使用不同的參數調用不同的內核，並且您可能會手動進行更多更改。

簡要的說：llm.c是訓練GPT-2的直接實現。 這個實現結果出乎意料地短。 不支援其他神經網絡，僅支援 GPT-2，如果您想更改網絡的任何內容，則需要專業知識。 幸運的是，所有最先進的 LLM 實際上與 GPT-2 根本沒有太大的區別，因此這並不像您想像的那麼嚴格。 並且 llm.c 必須進行額外的調整和完善，但原則上我認為它應該能夠幾乎匹配（甚至超越，因為我們擺脫了所有開銷？）PyTorch，代碼不會比現代 LLM 多太多。

我為什麼要從事這份工作？ 因為這很有趣。 它也很有教育意義，因為只需要那 1,000 行非常簡單的 C 程式碼，沒有別的。 它只是一些數字數組和對其元素進行一些簡單的數學運算，例如 + 和 *。 對於正在進行的更多工作，它甚至可能變得實際有用。
